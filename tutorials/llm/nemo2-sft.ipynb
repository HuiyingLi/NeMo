{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Goals\n",
    "\n",
    "## Supervised Finetuning (SFT)\n",
    "\n",
    "Often we want to adapt or customize foundation models to be more performant on our specific task. Fine-tuning refers to how we can modify the weights of a pre-trained foundation model with additional custom data. Supervised fine-tuning (SFT) refers to unfreezing all the weights and layers in our model and training on a newly labeled set of examples. We can fine-tune to incorporate new, domain-specific knowledge, or teach the foundation model what type of response to provide. One specific type of SFT is also referred to as “instruction tuning” where we use SFT to teach a model to follow instructions better. In this playbook will demostrate how to perform SFT with Llama3-8b using NeMo 2.0.\n",
    "\n",
    "## NeMo2.0\n",
    "\n",
    "In NeMo 1.0, the main interface for configuring experiments is through YAML files. This approach allows for a declarative way to set up experiments, but it has limitations in terms of flexibility and programmatic control. NeMo 2.0 is an update on the NeMo Framework which introduces several significant improvements over its predecessor, NeMo 1.0, enhancing flexibility, performance, and scalability.\n",
    "\n",
    "- Python-Based Configuration - NeMo 2.0 transitions from YAML files to a Python-based configuration, providing more flexibility and control. This shift makes it easier to extend and customize configurations programmatically.\n",
    "\n",
    "- Modular Abstractions - By adopting PyTorch Lightning’s modular abstractions, NeMo 2.0 simplifies adaptation and experimentation. This modular approach allows developers to more easily modify and experiment with different components of their models.\n",
    "\n",
    "- Scalability - NeMo 2.0 seamlessly scaling large-scale experiments across thousands of GPUs using NeMo-Run, a powerful tool designed to streamline the configuration, execution, and management of machine learning experiments across computing environments.\n",
    "\n",
    "By adopting PyTorch Lightning’s modular abstractions, NeMo 2.0 makes it easy for users to adapt the framework to their specific use cases and experiment with various configurations. This section offers an overview of the new features in NeMo 2.0 and includes a migration guide with step-by-step instructions for transitioning your models from NeMo 1.0 to NeMo 2.0.\n",
    "\n",
    "\n",
    "## Software Requirements\n",
    "\n",
    "1. Use the latest [NeMo Framework Training container](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/nemo/tags) . Note that you must be logged in to the container registry to view this page.\n",
    "\n",
    "2. This notebook uses the container: `nvcr.io/nvidia/nemo:dev`  #TODO: FIX CONTAINER  \n",
    "\n",
    "\n",
    "## Hardware Requirements\n",
    "\n",
    "- Minimum 8xA100 80G (1 node) for SFT on 7B and 13B\n",
    "\n",
    "- SFT can be run on all (7B/13B/70B) model sizes on multiple nodes\n",
    "\n",
    "\n",
    "## Data\n",
    "Databricks-dolly-15k is an open-source dataset created by the collaborative efforts of Databricks employees. It consists of high-quality human-generated prompt/response pairs specifically designed for instruction tuning LLMs. These pairs cover a diverse range of behaviors, from brainstorming and content generation to information extraction and summarization. \n",
    "\n",
    "For more information, refer to [databricks-dolly-15k | Hugging Face](https://huggingface.co/datasets/databricks/databricks-dolly-15k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0: Go inside docker container\n",
    "\n",
    "Here is a demo of starting and go inside the container on DGX Cloud. '\n",
    "\n",
    "Otherwise, you can start and enter the dev container by:  #TODO: FIX CONTAINER\n",
    "```\n",
    "docker run --gpus device=1 --shm-size=2g --net=host --ulimit memlock=-1 --rm -it -v ${PWD}:/workspace -w /workspace -v ${PWD}/results:/results nvcr.io/nvidia/nemo:dev bash\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Step 1: Import HuggingFace checkpoint\n",
    "First request download permission from Meta and Hugging Face. Login through `huggingface-cli` using your Huggingface token before importing llama3 models. \n",
    "\n",
    "```\n",
    "$ huggingface-cli login\n",
    "```\n",
    "\n",
    "Once you are logged in, NeMo 2.0 will automatically import the Hugging Face model and start training. There is no need to manully convert to NeMo checkpoint format.\n",
    "\n",
    "Let's first import needed python modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo import lightning as nl\n",
    "from nemo.collections import llm\n",
    "from megatron.core.optimizer import OptimizerConfig\n",
    "import torch\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Prepare data and customize DataModule\n",
    "\n",
    "We will be using Databricks-dolly-15k for this notebook. NeMo 2.0 already provides a `DollyDataModule`. Example usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def dolly() -> pl.LightningDataModule:\n",
    "    return llm.DollyDataModule(seq_length=2048, micro_batch_size=2, global_batch_size=8, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use your own data, you will need to create a custom `DataModule`. This involvse extending the base class `FineTuningDataModule`, so that you have access to existing data handling logic such as packed sequence. Here we walk you through the process step by step using the already existing [`DollyDataModule`](https://github.com/NVIDIA/NeMo/blob/main/nemo/collections/llm/gpt/data/dolly.py) as an example. \n",
    "\n",
    "### 1. Subclass the FineTuningDataModule\n",
    "You need to extend `FineTuningDataModule` if you're fine-tuning NeMo models. This provides access to existing data handling logic, such as packed sequences and tokenization. The `data_root` parameter is where you store your generated `train/validation/test.jsonl` in NeMo format. How `DollyDataModule` does it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from typing import TYPE_CHECKING, List, Optional\n",
    "from nemo.collections.common.tokenizers import TokenizerSpec\n",
    "from nemo.lightning.io.mixin import IOMixin\n",
    "from nemo.collections.llm.gpt.data.fine_tuning import FineTuningDataModule\n",
    "import json\n",
    "from nemo.utils import logging\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "class DollyDataModule(FineTuningDataModule, IOMixin):\n",
    "    def __init__(\n",
    "        self,\n",
    "        seq_length: int = 2048,\n",
    "        tokenizer: Optional[\"TokenizerSpec\"] = None,\n",
    "        micro_batch_size: int = 4,\n",
    "        global_batch_size: int = 8,\n",
    "        rampup_batch_size: Optional[List[int]] = None,\n",
    "        force_redownload: bool = False,\n",
    "        delete_raw: bool = True,\n",
    "        seed: int = 1234,\n",
    "        memmap_workers: int = 1,\n",
    "        num_workers: int = 8,\n",
    "        pin_memory: bool = True,\n",
    "        persistent_workers: bool = False,\n",
    "        pad_to_max_length: bool = False,\n",
    "        packed_sequence_size: int = -1,\n",
    "    ):\n",
    "        self.force_redownload = force_redownload\n",
    "        self.delete_raw = delete_raw\n",
    "\n",
    "        super().__init__(\n",
    "            dataset_root=get_dataset_root(\"dolly\"),\n",
    "            seq_length=seq_length,\n",
    "            tokenizer=tokenizer,\n",
    "            micro_batch_size=micro_batch_size,\n",
    "            global_batch_size=global_batch_size,\n",
    "            rampup_batch_size=rampup_batch_size,\n",
    "            seed=seed,\n",
    "            memmap_workers=memmap_workers,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=pin_memory,\n",
    "            persistent_workers=persistent_workers,\n",
    "            pad_to_max_length=pad_to_max_length,\n",
    "            packed_sequence_size=packed_sequence_size,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Override the `prepare_data` Method\n",
    "\n",
    "The prepare_data method is responsible for downloading and preprocessing data if needed. If the dataset is already downloaded, you can skip this step.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(self) -> None:\n",
    "    # if train file is specified, no need to do anything\n",
    "    if not self.train_path.exists() or self.force_redownload:\n",
    "        dset = self._download_data()\n",
    "        self._preprocess_and_split_data(dset)\n",
    "    super().prepare_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Implement Data Download and Preprocessing Logic\n",
    "\n",
    "If your dataset requires downloading or preprocessing, implement this logic within helper methods. Skip the download part if it's not needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _download_data(self):\n",
    "    logging.info(f\"Downloading {self.__class__.__name__}...\")\n",
    "    return load_dataset(\n",
    "        \"databricks/databricks-dolly-15k\",\n",
    "        cache_dir=str(self.dataset_root),\n",
    "        download_mode=\"force_redownload\" if self.force_redownload else None,\n",
    "    )\n",
    "\n",
    "def _preprocess_and_split_data(self, dset, train_ratio: float = 0.80, val_ratio: float = 0.15):\n",
    "    logging.info(f\"Preprocessing {self.__class__.__name__} to jsonl format and splitting...\")\n",
    "\n",
    "    test_ratio = 1 - train_ratio - val_ratio\n",
    "    save_splits = {}\n",
    "    dataset = dset.get('train')\n",
    "    split_dataset = dataset.train_test_split(test_size=val_ratio + test_ratio, seed=self.seed)\n",
    "    split_dataset2 = split_dataset['test'].train_test_split(\n",
    "        test_size=test_ratio / (val_ratio + test_ratio), seed=self.seed\n",
    "    )\n",
    "    save_splits['training'] = split_dataset['train']\n",
    "    save_splits['validation'] = split_dataset2['train']\n",
    "    save_splits['test'] = split_dataset2['test']\n",
    "\n",
    "    for split_name, dataset in save_splits.items():\n",
    "        output_file = self.dataset_root / f\"{split_name}.jsonl\"\n",
    "        with output_file.open(\"w\", encoding=\"utf-8\") as f:\n",
    "            for example in dataset:\n",
    "                context = example[\"context\"].strip()\n",
    "                if context != \"\":\n",
    "                    # Randomize context and instruction order.\n",
    "                    context_first = np.random.randint(0, 2) == 0\n",
    "                    if context_first:\n",
    "                        instruction = example[\"instruction\"].strip()\n",
    "                        assert instruction != \"\"\n",
    "                        _input = f\"{context}\\n\\n{instruction}\"\n",
    "                        _output = example[\"response\"]\n",
    "                    else:\n",
    "                        instruction = example[\"instruction\"].strip()\n",
    "                        assert instruction != \"\"\n",
    "                        _input = f\"{instruction}\\n\\n{context}\"\n",
    "                        _output = example[\"response\"]\n",
    "                else:\n",
    "                    _input = example[\"instruction\"]\n",
    "                    _output = example[\"response\"]\n",
    "\n",
    "                f.write(json.dumps({\"input\": _input, \"output\": _output, \"category\": example[\"category\"]}) + \"\\n\")\n",
    "\n",
    "        logging.info(f\"{split_name} split saved to {output_file}\")\n",
    "\n",
    "    if self.delete_raw:\n",
    "        for p in self.dataset_root.iterdir():\n",
    "            if p.is_dir():\n",
    "                shutil.rmtree(p)\n",
    "            elif '.jsonl' not in str(p.name):\n",
    "                p.unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original example in Dolly dataset looks like:\n",
    "```\n",
    "{'instruction': 'Extract all the movies from this passage and the year they were released out. Write each movie as a separate sentence', 'context': \"The genre has existed since the early years of silent cinema, when Georges Melies' A Trip to the Moon (1902) employed trick photography effects. The next major example (first in feature length in the genre) was the film Metropolis (1927). From the 1930s to the 1950s, the genre consisted mainly of low-budget B movies. After Stanley Kubrick's landmark 2001: A Space Odyssey (1968), the science fiction film genre was taken more seriously. In the late 1970s, big-budget science fiction films filled with special effects became popular with audiences after the success of Star Wars (1977) and paved the way for the blockbuster hits of subsequent decades.\", 'response': 'A Trip to the Moon was released in 1902. Metropolis came out in 1927. 2001: A Space Odyssey was released in 1968. Star Wars came out in 1977.', 'category': 'information_extraction'}\n",
    "```\n",
    "After the preprocessing logic, the data examples are transformed into NeMo format, as below:\n",
    "```\n",
    "{'input': \"Extract all the movies from this passage and the year they were released out. Write each movie as a separate sentence\\n\\nThe genre has existed since the early years of silent cinema, when Georges Melies' A Trip to the Moon (1902) employed trick photography effects. The next major example (first in feature length in the genre) was the film Metropolis (1927). From the 1930s to the 1950s, the genre consisted mainly of low-budget B movies. After Stanley Kubrick's landmark 2001: A Space Odyssey (1968), the science fiction film genre was taken more seriously. In the late 1970s, big-budget science fiction films filled with special effects became popular with audiences after the success of Star Wars (1977) and paved the way for the blockbuster hits of subsequent decades.\", 'output': 'A Trip to the Moon was released in 1902. Metropolis came out in 1927. 2001: A Space Odyssey was released in 1968. Star Wars came out in 1977.', 'category': 'information_extraction'}\n",
    "```\n",
    "Each data example is saved as a json string as one line in the `train/validation/test.jsonl` file, under `data_root` directory you specified earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Run SFT with NeMo 2.0 API \n",
    "\n",
    "The following python script utilizes NeMo 2.0 API to perform SFT. In this script we are configuring the following components for training. These components are similar between SFT and PEFT. SFT and PEFT both uses `llm.finetune` API. To switch from PEFT to SFT you just need to remove `peft` parameter.\n",
    "\n",
    "### Trainer\n",
    "NeMo 2.0 Trainer works simiarly to Pytorch Lightning trainer. You can specify to use MegatronStrategy as your model parallel strategy to use NVIDIA's Megatron-LM framework and pass in configurations as below:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def trainer(devices=1) -> nl.Trainer:\n",
    "    strategy = nl.MegatronStrategy(\n",
    "        tensor_model_parallel_size=1,\n",
    "    )\n",
    "\n",
    "    return nl.Trainer(\n",
    "        devices=devices,\n",
    "        max_steps=40,\n",
    "        accelerator=\"gpu\",\n",
    "        strategy=strategy,\n",
    "        plugins=nl.MegatronMixedPrecision(precision=\"bf16-mixed\"),\n",
    "        log_every_n_steps=1,\n",
    "        limit_val_batches=2,\n",
    "        val_check_interval=2,\n",
    "        num_sanity_val_steps=0,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Logger\n",
    "Configure your training steps, output directories and logging through `NeMoLogger`. In the following example, the experiment output will be saved at `./results/nemo2_sft`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logger() -> nl.NeMoLogger:\n",
    "    ckpt = nl.ModelCheckpoint(\n",
    "        save_last=True,\n",
    "        every_n_train_steps=10,\n",
    "        monitor=\"reduced_train_loss\",\n",
    "        save_top_k=1,\n",
    "        save_on_train_epoch_end=True,\n",
    "        save_optim_on_train_end=True,\n",
    "    )\n",
    "\n",
    "    return nl.NeMoLogger(\n",
    "        name=\"nemo2_sft\",\n",
    "        log_dir=\"./results\",\n",
    "        use_datetime_version=False,\n",
    "        ckpt=ckpt,\n",
    "        wandb=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Optimizer\n",
    "In the following example, we will be using distributed adam optimizer, and pass in optimizer configuration through `OptimizerConfig`: \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adam_with_cosine_annealing() -> nl.OptimizerModule:\n",
    "    return nl.MegatronOptimizerModule(\n",
    "        config=OptimizerConfig(\n",
    "            optimizer=\"adam\",\n",
    "            lr=0.0001,\n",
    "            adam_beta2=0.98,\n",
    "            use_distributed_optimizer=True,\n",
    "            clip_grad=1.0,\n",
    "            bf16=True,\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Model\n",
    "We will perform SFT on top of Llama3-8b so we create a `LlamaModel` to pass to finetune API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llama3_8b() -> pl.LightningModule:\n",
    "    from transformers import AutoTokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\n",
    "    return llm.LlamaModel(llm.Llama3Config8B(), tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoResume\n",
    "In NeMo 2.0 we can directly pass in Llama3-8b's Hugging Face ID to start SFT without manually converting it into NeMo checkpoint format like in NeMo 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def resume() -> nl.AutoResume:\n",
    "    return nl.AutoResume(\n",
    "        restore_config=nl.RestoreConfig(\n",
    "            path=\"hf://meta-llama/Meta-Llama-3-8B\"\n",
    "        ),\n",
    "        resume_if_exists=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### NeMo 2.0 finetun API\n",
    "Using all the components we created above, we can call NeMo 2.0 finetun API:\n",
    "```\n",
    "llm.finetune(\n",
    "    model=llama3_8b(),\n",
    "    data=dolly(),\n",
    "    trainer=trainer(),\n",
    "    log=logger(),\n",
    "    optim=adam_with_cosine_annealing(),\n",
    "    resume=resume(),\n",
    ")\n",
    "```\n",
    "Below is a python script that you can save as a file e.g. `nemo2-sft.py`, and run SFT training, using all components we created above and NeMo 2.0 finetune API. The script cannot be directly executed in interactive environment like a notebook. We can execute by `torchrun --nproc_per_node=<NUM_GPU> nemo2-sft.py` when multiple GPU is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo import lightning as nl\n",
    "from nemo.collections import llm\n",
    "from megatron.core.optimizer import OptimizerConfig\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "\n",
    "def trainer(devices=1) -> nl.Trainer:\n",
    "    strategy = nl.MegatronStrategy(\n",
    "        tensor_model_parallel_size=1,\n",
    "    )\n",
    "\n",
    "    return nl.Trainer(\n",
    "        devices=devices,\n",
    "        max_steps=40,\n",
    "        accelerator=\"gpu\",\n",
    "        strategy=strategy,\n",
    "        plugins=nl.MegatronMixedPrecision(precision=\"bf16-mixed\"),\n",
    "        log_every_n_steps=1,\n",
    "        limit_val_batches=2,\n",
    "        val_check_interval=2,\n",
    "        num_sanity_val_steps=0,\n",
    "    )\n",
    "\n",
    "\n",
    "def logger() -> nl.NeMoLogger:\n",
    "    ckpt = nl.ModelCheckpoint(\n",
    "        save_last=True,\n",
    "        every_n_train_steps=10,\n",
    "        monitor=\"reduced_train_loss\",\n",
    "        save_top_k=1,\n",
    "        save_on_train_epoch_end=True,\n",
    "        save_optim_on_train_end=True,\n",
    "    )\n",
    "\n",
    "    return nl.NeMoLogger(\n",
    "        name=\"nemo2_sft\",\n",
    "        log_dir=\"./results\",\n",
    "        use_datetime_version=False,\n",
    "        ckpt=ckpt,\n",
    "        wandb=None\n",
    "    )\n",
    "\n",
    "\n",
    "def adam_with_cosine_annealing() -> nl.OptimizerModule:\n",
    "    return nl.MegatronOptimizerModule(\n",
    "        config=OptimizerConfig(\n",
    "            optimizer=\"adam\",\n",
    "            lr=0.0001,\n",
    "            adam_beta2=0.98,\n",
    "            use_distributed_optimizer=True,\n",
    "            clip_grad=1.0,\n",
    "            bf16=True,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "def dolly() -> pl.LightningDataModule:\n",
    "    return llm.DollyDataModule(seq_length=2048, micro_batch_size=2, global_batch_size=8, num_workers=0)\n",
    "\n",
    "\n",
    "\n",
    "def llama3_8b() -> pl.LightningModule:\n",
    "    from transformers import AutoTokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\n",
    "    return llm.LlamaModel(llm.Llama3Config8B(), tokenizer=tokenizer)\n",
    "\n",
    "def resume() -> nl.AutoResume:\n",
    "    return nl.AutoResume(\n",
    "        restore_config=nl.RestoreConfig(\n",
    "            path=\"hf://meta-llama/Meta-Llama-3-8B\"\n",
    "        ),\n",
    "        resume_if_exists=True,\n",
    "    )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    llm.finetune(\n",
    "        model=llama3_8b(),\n",
    "        data=dolly(),\n",
    "        trainer=trainer(),\n",
    "        log=logger(),\n",
    "        optim=adam_with_cosine_annealing(),\n",
    "        resume=resume(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 Evaluation ##TODO: depending on NeMo 2.0 llm generation API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Launch with [NeMoRun](https://github.com/NVIDIA/NeMo-Run)\n",
    "Alternatively, we could use launch SFT jobs using existing [recipes](https://github.com/NVIDIA/NeMo/tree/main/nemo/collections/llm/recipes) from NeMo-Run. A recipe in NeMo is a python file that defines a complete configuration for training or fine-tuning an LLM. Each recipe typically includes:\n",
    "1. Model configuration: Defines the architecture and hyperparameters of the LLM.\n",
    "2. Training configuration: Specifies settings for the PyTorch Lightning Trainer, including distributed training strategies.\n",
    "3. Data configuration: Sets up the data pipeline, including batch sizes and sequence lengths.\n",
    "4. Optimization configuration: Defines the optimizer and learning rate schedule.\n",
    "5. Logging and checkpointing configuration: Specifies how to save model checkpoints and log training progress.\n",
    "\n",
    "Recipes are designed to be modular and extensible, allowing users to easily customize settings for their specific use cases.\n",
    "\n",
    "\n",
    "NeMo-Run is a powerful tool designed to streamline the configuration, execution, and management of machine learning experiments across various computing environments. NeMo-Run is responsible for experiment configuration, execution and management. Here is an example for launch a recipe using NeMo-Run using local executor. ##TODO: the default finetuning recipe include LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Pretrain with tp1pp1cp2 doesn't work. Pretrain with tp4pp1cp2 works. Finetuning recipe doesn't work\n",
    "import nemo_run as run\n",
    "from nemo.collections import llm\n",
    "\n",
    "recipe = llm.llama3_8b.finetune_recipe(name=\"llama3-8b-pretrain\", dir=\"exp/nemorun_ft\", num_nodes=1, num_gpus_per_node=2)\n",
    "env_vars = {\n",
    "    \"TORCH_NCCL_AVOID_RECORD_STREAMS\": \"1\",\n",
    "    \"NCCL_NVLS_ENABLE\": \"0\",\n",
    "    \"NVTE_DP_AMAX_REDUCE_INTERVAL\": \"0\",\n",
    "    \"NVTE_ASYNC_AMAX_REDUCTION\": \"1\",\n",
    "    \"NVTE_FUSED_ATTN\": \"0\",\n",
    "}\n",
    "local_executor = run.LocalExecutor(ntasks_per_node=8, launcher=\"torchrun\", env_vars=env_vars)\n",
    "run.run(recipe, executor=local_executor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
