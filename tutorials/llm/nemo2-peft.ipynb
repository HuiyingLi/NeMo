{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Goals\n",
    "\n",
    "## Parameter-Efficient Fine-Tuning (PEFT)\n",
    "\n",
    "This notebook aims to demonstrate how to adapt or customize foundation models to improve performance on specific tasks using NeMo 2.0.\n",
    "\n",
    "This optimization process is known as fine-tuning, which involves adjusting the weights of a pre-trained foundation model with custom data.\n",
    "\n",
    "Considering that foundation models can be significantly large, a variant of fine-tuning has gained traction recently known as PEFT. PEFT encompasses several methods, including P-Tuning, LoRA, Adapters, IA3, etc. NeMo 2.0 currently supports Low-Rank Adaptation(LoRA) method.\n",
    "\n",
    "This playbook involves applying LoRA to the Llama3 using NeMo 2.0. \n",
    "\n",
    "## NeMo 2.0\n",
    "\n",
    "In NeMo 1.0, the main interface for configuring experiments is through YAML files. This approach allows for a declarative way to set up experiments, but it has limitations in terms of flexibility and programmatic control. NeMo 2.0 is an update on the NeMo Framework which introduces several significant improvements over its predecessor, NeMo 1.0, enhancing flexibility, performance, and scalability.\n",
    "\n",
    "- Python-Based Configuration - NeMo 2.0 transitions from YAML files to a Python-based configuration, providing more flexibility and control. This shift makes it easier to extend and customize configurations programmatically.\n",
    "\n",
    "- Modular Abstractions - By adopting PyTorch Lightning’s modular abstractions, NeMo 2.0 simplifies adaptation and experimentation. This modular approach allows developers to more easily modify and experiment with different components of their models.\n",
    "\n",
    "- Scalability - NeMo 2.0 seamlessly scaling large-scale experiments across thousands of GPUs using NeMo-Run, a powerful tool designed to streamline the configuration, execution, and management of machine learning experiments across computing environments.\n",
    "\n",
    "By adopting PyTorch Lightning’s modular abstractions, NeMo 2.0 makes it easy for users to adapt the framework to their specific use cases and experiment with various configurations. This section offers an overview of the new features in NeMo 2.0 and includes a migration guide with step-by-step instructions for transitioning your models from NeMo 1.0 to NeMo 2.0.\n",
    "\n",
    "\n",
    "# NeMo Tools and Resources\n",
    "1. [NeMo Github repo](https://github.com/NVIDIA/NeMo)\n",
    "\n",
    "2. NeMo Framework Training container: `nvcr.io/nvidia/nemo:dev`  #TODO: FIX CONTAINER\n",
    "\n",
    "# Educational Resources\n",
    "1. Blog: [Mastering LLM Techniques: Customization](https://developer.nvidia.com/blog/selecting-large-language-model-customization-techniques/)\n",
    "\n",
    "2. Whitepaper: [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)\n",
    "\n",
    "3. [NeMo 2.0 Overview](https://docs.nvidia.com/nemo-framework/user-guide/latest/nemo-2.0/index.html)\n",
    "\n",
    "4. Blog: [Tune and Deploy LoRA LLMs with NVIDIA TensorRT-LLM](https://developer.nvidia.com/blog/tune-and-deploy-lora-llms-with-nvidia-tensorrt-llm/)\n",
    "\n",
    "\n",
    "## Software Requirements\n",
    "\n",
    "1. Use the latest [NeMo Framework Training container](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/nemo/tags) . Note that you must be logged in to the container registry to view this page.\n",
    "\n",
    "2. This notebook uses the container: `nvcr.io/nvidia/nemo:dev`  #TODO: FIX CONTAINER  \n",
    "\n",
    "\n",
    "## Hardware Requirements\n",
    "Llama3 8B: minimum 1xA100 80G\n",
    "\n",
    "\n",
    "## Data\n",
    "This notebook uses the SQUAD dataset. For more details about the data refer to [SQuAD: 100,000+ Questions for Machine Comprehension of Text](https://arxiv.org/abs/1606.05250)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0: Go inside docker container\n",
    "\n",
    "Here is a demo of starting and go inside the container on DGX Cloud. '\n",
    "\n",
    "Otherwise, you can start and enter the dev container by:  #TODO: FIX CONTAINER\n",
    "```\n",
    "docker run --gpus device=1 --shm-size=2g --net=host --ulimit memlock=-1 --rm -it -v ${PWD}:/workspace -w /workspace -v ${PWD}/results:/results nvcr.io/nvidia/nemo:dev bash\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Step 1: Import HuggingFace checkpoint\n",
    "First request download permission from Meta and Hugging Face. Login through `huggingface-cli` using your Huggingface token before importing llama3 models. \n",
    "\n",
    "```\n",
    "$ huggingface-cli login\n",
    "```\n",
    "\n",
    "Once you are logged in, NeMo 2.0 will automatically import the Hugging Face model and start training. There is no need to manully convert to NeMo checkpoint format.\n",
    "\n",
    "Let's first import needed python modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo import lightning as nl\n",
    "from nemo.collections import llm\n",
    "from megatron.core.optimizer import OptimizerConfig\n",
    "import torch\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Prepare data\n",
    "\n",
    "We will be using SQUAD for this notebook. NeMo 2.0 already provides a `SquadDataModule`. Example usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def squad() -> pl.LightningDataModule:\n",
    "    return llm.SquadDataModule(seq_length=2048, micro_batch_size=2, global_batch_size=8, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For how to use your own data to create your custom `DataModule` in order to perform PEFT, refer to [NeMo 2.0 SFT notebook](https://github.com/NVIDIA/NeMo/tree/main/tutorials/llm/nemo2-sft.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Run PEFT with NeMo 2.0 API \n",
    "\n",
    "The following python script utilizes NeMo 2.0 API to perform PEFT. In this script we are configuring the following components for training. These components are similar between SFT and PEFT. SFT and PEFT both uses `llm.finetune` API. To switch from SFT to PEFT you just need to add `peft` with LoRA adater to the API parameter.\n",
    "\n",
    "### Trainer\n",
    "NeMo 2.0 Trainer works simiarly to Pytorch Lightning trainer. You can specify to use MegatronStrategy as your model parallel strategy to use NVIDIA's Megatron-LM framework and pass in configurations as below:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def trainer(devices=1) -> nl.Trainer:\n",
    "    strategy = nl.MegatronStrategy(\n",
    "        tensor_model_parallel_size=1,\n",
    "    )\n",
    "\n",
    "    return nl.Trainer(\n",
    "        devices=1,\n",
    "        max_steps=40,\n",
    "        accelerator=\"gpu\",\n",
    "        strategy=strategy,\n",
    "        plugins=nl.MegatronMixedPrecision(precision=\"bf16-mixed\"),\n",
    "        log_every_n_steps=1,\n",
    "        limit_val_batches=2,\n",
    "        val_check_interval=2,\n",
    "        num_sanity_val_steps=0,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logger\n",
    "Configure your training steps, output directories and logging through `NeMoLogger`. In the following example, the experiment output will be saved at `./results/nemo2_peft`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logger() -> nl.NeMoLogger:\n",
    "    ckpt = nl.ModelCheckpoint(\n",
    "        save_last=True,\n",
    "        every_n_train_steps=10,\n",
    "        monitor=\"reduced_train_loss\",\n",
    "        save_top_k=1,\n",
    "        save_on_train_epoch_end=True,\n",
    "        save_optim_on_train_end=True,\n",
    "    )\n",
    "\n",
    "    return nl.NeMoLogger(\n",
    "        name=\"nemo2_peft\",\n",
    "        log_dir=\"./results\",\n",
    "        use_datetime_version=False,\n",
    "        ckpt=ckpt,\n",
    "        wandb=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Optimizer\n",
    "In the following example, we will be using distributed adam optimizer, and pass in optimizer configuration through `OptimizerConfig`: \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adam_with_cosine_annealing() -> nl.OptimizerModule:\n",
    "    return nl.MegatronOptimizerModule(\n",
    "        config=OptimizerConfig(\n",
    "            optimizer=\"adam\",\n",
    "            lr=0.0001,\n",
    "            adam_beta2=0.98,\n",
    "            use_distributed_optimizer=True,\n",
    "            clip_grad=1.0,\n",
    "            bf16=True,\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoRA Adapter\n",
    "We need to pass in LoRA adapter to our finetuning API to perform LoRA finetuning. We can configure adapter like the following. The target module we support includes: `linear_qkv`, `linear_proj`, `linear_fc1` and `linear_fc2`. In the final script we used default configurations for LoRA (`llm.peft.LoRA()`), which will use the full list with `dim=32`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lora() -> nl.pytorch.callbacks.PEFT:\n",
    "    return llm.peft.LoRA(\n",
    "        target_modules=['linear_qkv', 'linear_proj'], # full list:['linear_qkv', 'linear_proj', 'linear_fc1', 'linear_fc2']\n",
    "        dim=32,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Model\n",
    "We will perform PEFT on top of Llama3-8b so we create a `LlamaModel` to pass to finetune API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llama3_8b() -> pl.LightningModule:\n",
    "    from transformers import AutoTokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\n",
    "    return llm.LlamaModel(llm.Llama3Config8B(), tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoResume\n",
    "In NeMo 2.0 we can directly pass in Llama3-8b's Hugging Face ID to start PEFT without manually converting it into NeMo checkpoint format like in NeMo 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def resume() -> nl.AutoResume:\n",
    "    return nl.AutoResume(\n",
    "        restore_config=nl.RestoreConfig(\n",
    "            path=\"hf://meta-llama/Meta-Llama-3-8B\"\n",
    "        ),\n",
    "        resume_if_exists=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### NeMo 2.0 finetun API\n",
    "Using all the components we created above, we can call NeMo 2.0 finetun API:\n",
    "```\n",
    "llm.finetune(\n",
    "    model=llama3_8b(),\n",
    "    data=squad(),\n",
    "    trainer=trainer(),\n",
    "    peft=lora(),\n",
    "    log=logger(),\n",
    "    optim=adam_with_cosine_annealing(),\n",
    "    resume=resume(),\n",
    ")\n",
    "```\n",
    "Below is a python script that you can save as a file e.g. `nemo2-peft.py`, and run PEFT training, using all components we created above and NeMo 2.0 finetune API. The script cannot be directly executed in interactive environment like a notebook. We can execute by `python nemo2-peft.py` if single GPU is used, or `torchrun --nproc_per_node=<NUM_GPU> nemo2-peft.py` if multiple GPU is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo import lightning as nl\n",
    "from nemo.collections import llm\n",
    "from megatron.core.optimizer import OptimizerConfig\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "\n",
    "def trainer(devices=1) -> nl.Trainer:\n",
    "    strategy = nl.MegatronStrategy(\n",
    "        tensor_model_parallel_size=1,\n",
    "    )\n",
    "\n",
    "    return nl.Trainer(\n",
    "        devices=1,\n",
    "        max_steps=40,\n",
    "        accelerator=\"gpu\",\n",
    "        strategy=strategy,\n",
    "        plugins=nl.MegatronMixedPrecision(precision=\"bf16-mixed\"),\n",
    "        log_every_n_steps=1,\n",
    "        limit_val_batches=2,\n",
    "        val_check_interval=2,\n",
    "        num_sanity_val_steps=0,\n",
    "    )\n",
    "\n",
    "\n",
    "def logger() -> nl.NeMoLogger:\n",
    "    ckpt = nl.ModelCheckpoint(\n",
    "        save_last=True,\n",
    "        every_n_train_steps=10,\n",
    "        monitor=\"reduced_train_loss\",\n",
    "        save_top_k=1,\n",
    "        save_on_train_epoch_end=True,\n",
    "        save_optim_on_train_end=True,\n",
    "    )\n",
    "\n",
    "    return nl.NeMoLogger(\n",
    "        name=\"nemo2_peft\",\n",
    "        log_dir=\"./results\",\n",
    "        use_datetime_version=False,\n",
    "        ckpt=ckpt,\n",
    "        wandb=None\n",
    "    )\n",
    "\n",
    "\n",
    "def adam_with_cosine_annealing() -> nl.OptimizerModule:\n",
    "    return nl.MegatronOptimizerModule(\n",
    "        config=OptimizerConfig(\n",
    "            optimizer=\"adam\",\n",
    "            lr=0.0001,\n",
    "            adam_beta2=0.98,\n",
    "            use_distributed_optimizer=True,\n",
    "            clip_grad=1.0,\n",
    "            bf16=True,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "def lora() -> nl.pytorch.callbacks.PEFT:\n",
    "    return llm.peft.LoRA()\n",
    "\n",
    "\n",
    "\n",
    "def squad() -> pl.LightningDataModule:\n",
    "    return llm.SquadDataModule(seq_length=2048, micro_batch_size=2, global_batch_size=8, num_workers=0)\n",
    "\n",
    "\n",
    "\n",
    "def llama3_8b() -> pl.LightningModule:\n",
    "    from transformers import AutoTokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\n",
    "    return llm.LlamaModel(llm.Llama3Config8B(), tokenizer=tokenizer)\n",
    "\n",
    "def resume() -> nl.AutoResume:\n",
    "    return nl.AutoResume(\n",
    "        restore_config=nl.RestoreConfig(\n",
    "            path=\"hf://meta-llama/Meta-Llama-3-8B\"\n",
    "        ),\n",
    "        resume_if_exists=True,\n",
    "    )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    llm.finetune(\n",
    "        model=llama3_8b(),\n",
    "        data=squad(),\n",
    "        trainer=trainer(),\n",
    "        peft=lora(),\n",
    "        log=logger(),\n",
    "        optim=adam_with_cosine_annealing(),\n",
    "        resume=resume(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 Evaluation ##TODO: depending on NeMo 2.0 llm generation API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Launch with [NeMo-Run](https://github.com/NVIDIA/NeMo-Run)\n",
    "Alternatively, we could use launch PEFT jobs using existing [recipes](https://github.com/NVIDIA/NeMo/tree/main/nemo/collections/llm/recipes) from NeMo-Run. A recipe in NeMo is a python file that defines a complete configuration for training or fine-tuning an LLM. Each recipe typically includes:\n",
    "1. Model configuration: Defines the architecture and hyperparameters of the LLM.\n",
    "2. Training configuration: Specifies settings for the PyTorch Lightning Trainer, including distributed training strategies.\n",
    "3. Data configuration: Sets up the data pipeline, including batch sizes and sequence lengths.\n",
    "4. Optimization configuration: Defines the optimizer and learning rate schedule.\n",
    "5. Logging and checkpointing configuration: Specifies how to save model checkpoints and log training progress.\n",
    "\n",
    "Recipes are designed to be modular and extensible, allowing users to easily customize settings for their specific use cases.\n",
    "\n",
    "\n",
    "NeMo-Run is a powerful tool designed to streamline the configuration, execution, and management of machine learning experiments across various computing environments. NeMo-Run is responsible for experiment configuration, execution and management. Here is an example for launch a recipe using NeMo-Run using local executor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Pretrain with tp1pp1cp2 doesn't work. Pretrain with tp4pp1cp2 works. Finetuning recipe doesn't work\n",
    "import nemo_run as run\n",
    "from nemo.collections import llm\n",
    "\n",
    "recipe = llm.llama3_8b.finetune_recipe(name=\"llama3-8b-pretrain\", dir=\"exp/nemorun_ft\", num_nodes=1, num_gpus_per_node=2)\n",
    "env_vars = {\n",
    "    \"TORCH_NCCL_AVOID_RECORD_STREAMS\": \"1\",\n",
    "    \"NCCL_NVLS_ENABLE\": \"0\",\n",
    "    \"NVTE_DP_AMAX_REDUCE_INTERVAL\": \"0\",\n",
    "    \"NVTE_ASYNC_AMAX_REDUCTION\": \"1\",\n",
    "    \"NVTE_FUSED_ATTN\": \"0\",\n",
    "}\n",
    "local_executor = run.LocalExecutor(ntasks_per_node=8, launcher=\"torchrun\", env_vars=env_vars)\n",
    "run.run(recipe, executor=local_executor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
